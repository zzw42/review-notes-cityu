% chapter 3
% Last edit: 2017-5-4
% Modified in Version 1.02 -- 2018-1-18
\chapter{Discrete Random Variables}
\section{Random Variable}
\begin{defn}
  For a given sample space $\mathcal{S}$ of some experiment, a random variable (rv) is any rule that associates a number with each outcome in $\mathcal{S}$. In mathematical language, a random variable is a function whose domain is the sample space
and whose range is the set of real numbers.
\end{defn}

\begin{exmp}
Flip a coin, $\mathcal{S}=\{\text{H} ,\text{T} \}$
\[X(H) = 1 \qquad X(T) =0 \]
\end{exmp}

\begin{exmp}
Randomly pick a student, height
\[X(\text{height}\geq \text{6 feet})=1  \qquad X(\text{height}\leq \text{6 feet})=0
\]
\end{exmp}

\begin{defn}
Any r.v. whose possible values are 0 and 1 is called a \textbf{Bernoulli random variable}.
\end{defn}

\begin{exmp}
Randomly pick a student, phone brand
\[X(\text{Apple})=1 \qquad  X(\text{Samsung})=0	\]
\end{exmp}

\begin{exmp}
Waiting MTR at Kowloon Tong
\[X(\text{waiting time})=\text{waiting time}\]
\end{exmp}

\subsection{Two Types of Random Variables}
\begin{defn}
a \textbf{discrete} r.v. whose possible values are either finite or countable.

a \textbf{continuous} r.v. is a r.v. whose possible values consist of an entire interval on the real lines.
\end{defn}

\section{Probability Distributions for Discrete Random Variables}

\begin{defn}

$\mathcal{S}$ is a sample space, $X(s)$ is a r.v. $p(x)=P(s\in \mathcal{S};X(s)= x) $ is called the probability mass function (p.m.f) or probability distribution function (p.d.f) of $x$.
\end{defn}

\begin{exmp}
\[	\mathcal{S}=(5 \text{ feet}, 7 \text{ feet} )		\]
\[	X(s)=\begin{cases}
1, & \text{if} \quad s\geq 6 \text{ feet} \\
0. & \text{if} \quad s\leq 6 \text{ feet}
\end{cases}		\]
\[	P(X=1)=P(s \geq 6 \text{ feet})\]
\end{exmp}

\begin{exmp}
Six lots of components that the \# of defectives are listed as follows
\begin{center}
\begin{tabular}{c|cccccc}
\hline
lot				& 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
\# of defectives& 0 & 2 & 0 & 1 & 2 & 0 \\
\hline
\end{tabular}
\end{center}

One of those is randomly selected. $X$ = \# of defectives in the selected lot
\[	P(X=0)=P(\{1,3,6\})=\frac{1}{2}		\]
\[	P(X=1)=P(\{4\})=\frac{1}{6}			\]
\[	P(X=2)=P(\{2,5\})=\frac{1}{3}		\]
\end{exmp}

\begin{exmp}
Five person 1,2,3,4,5 are blood donors. Among them, only 1 and 2 have ``O" type. Collect their blood in a random segment, $X=\text{\# of typing neccessary to get the first ``O" type}$.
\[	X = 1,2,3,4		\]
\[	P(X=1)=P(\text{typing after the first trail})=\frac{2}{5} \]
\end{exmp}

\subsubsection{Review}
$X$ is a discrete r.v.
\begin{enumerate}
\item Support $x \in \mathcal{D}$
\item p.m.f $p(x)=P(s\in \mathcal{S};X(s)= x), \quad\forall x \in \mathcal{D}$
\end{enumerate}


\subsection{The Cumulative Distribution Function}
\begin{exmp}
Roll a dice. Let $x$= \# of spots. What is the probability that $x \leq 5$
\[	\mathcal{D}=\{1,2,3,4,5,6\}		\]
\[	P(1)=P(2)=\dots=P(6)=\frac{1}{6}		\]
\[	P(X \leq 5)=P(\{1,2,3,4,5\})=P(1)+P(2)+P(3)+P(4)+P(5)=\frac{5}{6}	\]
\[F(x)=\begin{cases}
P(X \leq x)=0 &\text{if } x < 1 \\
P(X \leq x)=\frac{1}{6} &\text{if } 1 \leq x < 2 \\
\dots \\
P(X \leq x)=1 &\text{if } x \geq 6 \\
\end{cases}\]

It is called step function.
\end{exmp}

\begin{defn}
The Cumulative Distribution Function (c.d.f) of a r.v $X$ is defined as 
\[	F(x)=P(X\leq x)=\sum_{y\leq x}p(y) \]
\end{defn}

\begin{exmp}
a r.v $Y$

\begin{center}
\begin{tabular}{c|cccc}
\hline
$y$  & 1 & 2 & 3 & 4\\
\hline
$p(y)$  & 0.4 & 0.3  & 0.2  & 0.1 \\
\hline
\end{tabular}
\end{center}


\[F(y)=P(Y\leq y)=\begin{cases}
0 &\text{if } y < 1 \\
0.4 &\text{if } 1 \leq y < 2 \\
0.7 &\text{if } 2 \leq y < 3 \\
0.9 &\text{if } 3 \leq y < 4 \\
1 &\text{if } y \geq 4 \\
\end{cases}
\]
\end{exmp}

\begin{exmp}
Toss a coin until the first head. Suppose $P(\text{Head})=p$, $P(\text{Tail})=q=1-p$, $x=\text{\# of toses until the first head}$
\[\mathcal{D}=\{1,2,3,\dots\}\]
\[p(x)=q^{x-1}p, \qquad x=1,2,3,\dots\]
\[F(x)=P(X \leq x)=\sum_{y \leq x}p(y)=\sum_{y \leq x}q^{y-1}p=p \frac{1-q^{\floor*{x}}}{1-q}=1-q^{\floor*{x}}\]

where $\floor*{x}$ is the largest integer $\leq x$ (floor function).
\[F(x)=\begin{cases}
0, & \text{if } x <0 \\
1-q^{\floor*{x}}. &\text{if } x\geq 0\\
\end{cases}\] 
\end{exmp}

\subsubsection{How do we get p.m.f from c.d.f}
In examples thus far, the cdf has been derived from the pmf. This process can be reversed to obtain the pmf from the cdf whenever the latter function is available.
\[P(X=3)=P(x\leq 3)-P(x \leq 2)=F(3)-F(2)\]

Suppose $X$ takes integer values, for any integers $a$ and $b$,
\[P(a\leq X \leq b)=P( X \leq b)-P(X \leq a-1)=F(b)-F(a-1)\]

Generally, for $a$ and $b$
\[P(a\leq X \leq b)=F(b)-F(a_-)\]

Here $a_-$ is the largest integer value that is strictly less than $a$. If $a=2$, $\floor*{a}=2$, $a_-=1$  

\section{Expected Values}
\begin{exmp} [``Russian roulette"]
Bet even or odd. Bet \$1 on even, I will win \$1 if indeed it is even, and I will lose \$1 if it is odd, or 0, or 00.

Expected value
\[	\frac{18}{38}\times1+\frac{20}{38}\times(-1)=-\frac{2}{38}	\]

\end{exmp}

\subsection{The Expected Value of $X$}
\begin{defn}
Let $X$ be a discrete rv with set of possible values $\mathcal{D}$ and pmf $p(x)$. The expected value or mean value of $X$, denoted by $E(X)$ or $\mu_X$ or just $\mu$, is
\[E(X)=\sum_{x\in \mathcal{D}} x p(x)\]
\[x=\begin{cases}
1,&\text{w.p.}\frac{18}{38} \\
-1,&\text{w.p.}\frac{20}{38}
\end{cases}\]
\[ E(X)=-\frac{2}{38}\]
\end{defn}

\begin{exmp}
$X$ is a Bernoulli r.v
\[p(x)=\begin{cases}
p, & \text{if} \quad x=1\\
1-p. & \text{if} \quad x=0
\end{cases}\]
\[E(X)=1 p+0 (1-p)=p\]
\end{exmp}

\begin{exmp}
A newly-wed couple want a girl. Their plan is to keep having children until they get a girl.
\[  X=\text{\# of children when the girl is born}   \]
\[  P(\text{boy})=p \qquad P(\text{girl})=1-p=q     \]
\[	p(x)=p^{x-1}q \qquad x=1,2,3,\dots	\]
\[	E(X)=\sum_{x=1}^{\infty} xp^{x-1}q= q\sum_{x=1}^{\infty}xp^{x-1}	\]
\[	S=p^0+2p^1+3p^2+4p^3+\dots \]
\[	pS=p^1+2p^2+3p^3+4p^4+\dots \]
\[	(1-p)S=p^0+p^1+p^2+p^3+p^4+\dots=\frac{1}{1-p}\]
\[	E(X)=q\frac{1}{(1-p)^2}=\frac{1}{q}\]

Another method to calculate $\sum_{x=1}^{\infty} xp^{x-1}q$
\[	\sum_{x=1}^{\infty} xp^{x-1}=\sum_{x=1}^{\infty}(p^{x})'=\Big( \sum_{x=1}^{\infty}p^{x} \Big)'	\]
\end{exmp}

\begin{exmp}
\[	p(k)=\frac{1}{k^2}\frac{6}{\pi^2}	\qquad k=1,2,3,\dots\]

Verify 
\[\sum_{k=1}^{\infty} p(k)=1\]
\[E(x)=\sum_{k=1}^{\infty} k\frac{1}{k^2}\frac{6}{\pi^2}		=\frac{6}{\pi^2}\sum_{k=1}^{\infty}\frac{1}{k}=\infty	\]
\end{exmp}


\subsection{The Expected Value of a Function}
\begin{prop}
\[E(h(X))=\sum_{x\in \mathcal{D}}h(x)p(x)\]
\end{prop}

\begin{exmp}
\# of cylinders in the engine of the next car to be turned up.

Cost for $x$ cylinders
\[	h(x)=20+3x+0.5 x^2		\]

History shows that 
\begin{center}
  \begin{tabular}{c|ccc}
  \hline
  $x$  & 4 & 6 & 8 \\
  \hline
  $p(x)$  & 0.5 & 0.3  & 0.2  \\
  $h(x)$  & 40 & 56 & 76 \\
  \hline
  \end{tabular}
\end{center}
  
 \vspace{4mm}

 \[	E(h(x))=40\times0.5 +56\times 0.3+76\times 0.2 = \boxed{52}\]
\end{exmp}


\subsection{Rules of Expected Value}
\begin{prop}
Let $a$ and $b$ be two constant, $X$ r.v
\[E(aX+b)=aE(X)+b\]

Particularly,
\[\text{if} \quad b=0, \quad E(aX)=aE(X)\]
\[\text{if} \quad a=0, \quad E(X+b)=E(X)+b\]
\end{prop}

\begin{exmp}
  A computer store has purchased three computers of a certain type at \$500 apiece. It will sell them for \$1000 apiece. The manufacturer has agreed to repurchase any computers still unsold after a specified period at \$ 200 apiece. Let $X$ denote the number of computers sold.

\begin{center}
\begin{tabular}{c|cccc}
\hline
$x$ & 0 & 1 & 2 & 3 \\
\hline
$p(x)$  & 0.1 & 0.2  & 0.3  & 0.4 \\
\hline
\end{tabular}
\qquad $E(X)=2$
\end{center}

\[	Y=1000X+200(3-X)-1500=800X-900\]
\[	E(Y)=800E(X)-900=700\]
\end{exmp}

\subsection{The Variance of $X$}
\begin{exmp}
\qquad
\begin{tabular}{c|cccccc}
\hline
$x$ & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
$p(x)$  & $\frac{1}{6}$ & $\frac{1}{6}$  & $\frac{1}{6}$  & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$\\
\hline
\end{tabular}

\[E(X)=\frac{7}{2}\]
  
\begin{center}
\begin{tabular}{c|cc}
\hline
$x$ &  3 & 4  \\
\hline
$p(x)$  & $\frac{1}{6}$ & $\frac{1}{6}$  \\
\hline
\end{tabular}
\end{center}

\[E(X)=\frac{7}{2}\]  
\end{exmp}

\begin{defn}
$X$ is a discrete random variable $E(X)=\mu$, 
\[\sigma_{x}^2=Var(X)=E((X-\mu)^2)=\sum_{x \in \mathcal{D}} (x-\mu)^2p(x)\]
\[\sigma_x=s.d(X)=\sqrt{Var(X)}\]
\end{defn}

For Ex(1), $Var(X)=2.92$; For Ex(2), $Var(X)=0.25$.


\subsection{Short-cut Formula}
\begin{prop}
\[Var(X)=E(X^2)-(E(X))^2\]
\begin{proof}
\begin{align*}
Var(X)=& E((X-\mu)^2)= E(x^2-2X\mu+\mu^2)\\
=&E(X^2)+E(-2X\mu)+E(\mu^2) = E(X^2)-2\mu E(X)+\mu^2 \\
=& E(X^2)-2\mu \mu+\mu^2 = E(X^2)-(E(X))^2
\end{align*}
\end{proof}
\end{prop}

\subsection{Rules}
\begin{prop}
\[Var(aX+b)=a^2 Var(X)\]
\[s.d(aX+b)=|a| s.d(X)\]

since $a$ could be negative.
\end{prop}

\begin{exmp}
Computer store
\[Y=800X-900\]

\begin{center}
\begin{tabular}{c|cccc}
\hline
$x$ & 0 & 1 & 2 & 3 \\
\hline
$p(x)$  & 0.1 & 0.2  & 0.3  & 0.4 \\
\hline
\end{tabular}
\end{center}

\[Var(Y)=Var(800X-900)=800^2 Var(X)\]
\[E(X)=2 \qquad E(X^2)=5\]
\[Var(Y)=800^2(5-2^2)=640000\]
\end{exmp}


\section{The Binomial Probability Distribution}
Recall $X \sim Bernobli(p)$
\[p(0)=1-p \qquad p(1)=p\]

\begin{exmp}
Flip a coin 3 times independently. 
$X$=\# of Heads. What's the distribution of $X$?
\[\mathcal{D}=\{0,1,2,3\}\]
\begin{center}
\begin{tabular}{r|c|l}
\hline
       &  $x$  & $p(x)$ \\
    \hline
    TTT & 0 & $(1-p)^3$ \\
    HTT, THT, TTH  & 1 & $3p(1-p)^2$ \\
    HHT, HTH, THH  & 2 & $3p^2 (1-p)$ \\
    HHH & 3 & $p^3$ \\
\hline
\end{tabular}
\[\sum p(x)=1\] 
\end{center}
\end{exmp}

\subsection{The Binomial Random Variable and Distribution}
Generally, $n$ Bernouli trails, independently. the success rate of each trail is constant $p$, then the \# of success out of these $n$ trails is a \textbf{Binomial} r.v, denoted as $X \sim Bin(n,p)$

If $X \sim Bin(n,p)$,

\[P(X=x)=\binom nx p^x (1-p)^{n-x}  \qquad x=0,1,\dots n\]

Back to the example, \[P(X=0)=\binom 30 p^0 (1-p)^3=(1-p)^3\]

\subsection{The Mean and Variance of $X$}
\begin{prop}
If $X \sim Bin(n,p)$,
\[E(X)=np \qquad Var(X)=np(1-p)\]
\begin{proof}
\begin{align*}
E(X)&=\sum_{k=0}^{n} k\binom nk p^k (1-p)^{n-k} =\sum_{k=0}^{n} k \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}\\
&=\sum_{k=1}^{n}  \frac{n \cdot(n-1)!}{(k-1)!(n-k)!} p^k (1-p)^{n-k} \\
&=np \sum_{k=1}^{n}  \frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} (1-p)^{n-k} \\
&=np \sum_{k=1}^{n}  \binom {n-1}{k-1} p^{k-1} (1-p)^{n-k}		\\
&=np \sum_{k'=0}^{n'}  \binom {n'}{k'} p^{k'} (1-p)^{n'-k'}=np
\end{align*}
\end{proof}
\end{prop}

\begin{exmp}
Six cola drinkers. Two brand: C, P. $X$ = \# of cola C they choose.
\[P(C)=\frac{1}{2} \qquad P(P)=\frac{1}{2}\]
\[X \sim Bin \left(6,\frac{1}{2} \right)\]
\begin{align*}
&P(X=3)=\binom 63 \left(\frac{1}{2}\right)^3	\left(1-\frac{1}{2}\right)^{6-3}=0.313 \\
&P(X \leq 1)=P(X=0)+P(X=1)=0.109 \\
&P(X \geq 3)=1-P(X \leq 2)
\end{align*}
\end{exmp}

\subsection{Using Binomial Tables}

\section{Hypergeometric and Negative Binomial\\ Distributions}
\begin{exmp}
5 balls in a box, 3 red, 2 blue. Randomly choose 3 balls out of the box with replacement. What is the chance of getting 2 red and 1 blue balls?
\[X= \text{\# of red balls out of 3}\]
\[X \sim  Bin\left(3,\frac{3}{5}\right)\]
\[P(X=2)=\binom 32 =\frac{54}{125}\]
\end{exmp}

\begin{exmp}
Same step. without replacement.
\[X= \text{\# of red balls out of 3}\]
\[X \quad  \not\sim  Bin\left(3,\frac{3}{5}\right)\]
\begin{align*}
P(X=2)=& \frac{\text{\# of outcome in } E}{\text{\# of outcomes in } \mathcal{S}} \\
=& \frac{\binom 32 \binom 21}{\binom 53} =\frac{3}{5}
\end{align*}
\end{exmp}

\subsection{Hypergeometric}
\begin{prop}
In general, $M$ of type ``1", $N-M$ of type ``2 in a box, choose $n$ items.
\[Y \sim \text{hypergeometric}(N,M,n)\]
\[P(Y=k)=\frac{\binom Mk \binom {N-M}{n-k}}{\binom N n}\qquad k=(0 \vee n-(N-M)),1,2,\dots,(n \wedge M)\]
\end{prop}

\subsection{The Mean and Variance of $X$}
\begin{prop}
If $X \sim \text{hypergeometric}(N,M,n)$,
\[E(X)=n \cdot \frac{M}{N}\]
\[Var(X)=\frac{N-n}{N-1}\cdot n \cdot\frac{M}{N}\left(1-\frac{M}{N}\right)\]
\end{prop}

\begin{exmp}
Five wolves are caught in a forest. Tagged and released to mix with other wolves. After a while, 10 wolves are caught.

Assume there are 25 such wolves in the forest. $P(X=2)=?$
\[X \sim h.g.(25,5,10)\]
\begin{align*}
&P(X=2)=\frac{\binom 52 \binom {20}{8}}{\binom {25}{10}}=0.385 \\
&E(X)=2	\\
&Var(X)=1
\end{align*}
If we have no idea about the number of wolves in the forest. But $X=3$, how to estimate the \# of wolves in the forest ?
\[N= \text{\# of wolves in total}\]
\[10 \cdot \frac{5}{N}=E(X)\approx 3\]
\[N \approx 10 \cdot \frac{5}{3}\approx17\]
\end{exmp}

\subsection{The Negative Binomial Distribution}
\begin{exmp}
A couple wants 3 girls. How many children they need to have to have fulfil his planning?
\[P(\text{girl})=p \qquad P(\text{boy})=1-p\]
\[\text{X=\# of children to attain this planning}\]
\[x\geq 3 \qquad \mathcal{D}=\{3,4,\dots\}\]
\[P(X=k)=\binom {k-1}{2} p^3(1-p)^{k-3}\]

This is called ``Negative binomial r.v"
\end{exmp}

\begin{prop}
In general, $X \sim \textit{Negative Binomial}(r,p)$
\[P(X=k)=\binom {k-1}{r-1} p^r (1-p)^{k-r}  \qquad k=r,r+1,\dots	\]
\[E(X)=\frac{r}{p} \qquad  Var(X)=\frac{r(1-p)}{p^2} \]
\end{prop}

\begin{exmp}
Roll a dice repeatedly until the first ``one" occurs. $X =$ \# of rollings.
\[X \sim n.b(1,\frac{1}{6})\]
\[E(X)=\frac{1}{\frac{1}{6}}=6  	\qquad Var(X)=30\]
\end{exmp}

\section{The Poisson Probability Distribution}
\begin{defn}
A r.v. $X$ takes value 0,1,2,3,\dots
 \[P(X=k)=\frac{\lambda^k}{k!} e^{-\lambda} \qquad k=0,1,2\dots\]

where $\lambda >0$. Then we say $X \sim Poisson(\lambda)$
\end{defn}

Check
 \[\sum_{k=0}^{\infty} \frac{\lambda^k}{k!} e^{-\lambda}=e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} =1 \]

Since 
\[e^{\lambda}=\sum_{k=0}^{\infty} \frac{\lambda^k}{k!}  \]

(Taylor expansion)
\subsection{The Mean and Variance of $X$}
\begin{prop}
If $X \sim Poisson(\lambda)$,$E(X)=\lambda$, $Var(X)=\lambda$
\begin{proof}
\begin{align*}
E(X)&=\sum _{k=0}^{\infty}kP(X=k)=\sum _{k=0}^{\infty}  k \frac{\lambda^k}{k!} e^{-\lambda} \\
&=\lambda \sum _{k=0}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} e^{-\lambda}=\lambda \sum _{k'=0}^{\infty} \frac{\lambda^{k'}}{k'!} e^{-\lambda}=\lambda
\end{align*}
\end{proof}
\end{prop}

\subsection{The Poisson Distribution as a Limit}
\begin{prop}
If $X \sim Bin(n,p)$, $n$ is large, $p$ is small. Then $X \sim Poisson(\lambda)$ with $\lambda=np$.
\end{prop}

\begin{exmp}
A publisher is publishing a non-technical book. P(making at least one error in a page)=0.005. The book has 400 pages, independent from page to page.\\
X=\# of pages with errors $\sim$ Bin(400,0.005)
\[P(X=2)=\binom {400}{2} 0.005^2 (1-0.005)^{400-2}\]
\[X \sim Poisson(2)\]
\[P(X=2)=\frac{2^2}{2!}e^{-2}=0.27\]
\end{exmp}

\subsubsection{Rule of Thumb}
When $n \geq 50$, $np \leq 5$, we consider $n$ is large enough, $p$ is small enough.

\subsection{The Poisson Process}
\begin{exmp}
Counting the number of customers at a bank counter. Suppose
\begin{enumerate}
\item $\exists \alpha > 0$ such that
\[P(\text{exact one customer in } \Delta t)=\alpha \Delta t +o(\Delta t)\]
\item \[P(\text{more than one customer in } \Delta t)=o(\Delta t)\]
\item Number of customers during $\Delta t$ is independent of that prior to this period
\end{enumerate}

Then $P(k$ customers during $(0,t))=\frac{(\alpha t)^k}{k!}e^{-\alpha t}$.
Let $X_t$ = \# of customers during $(0,t)$.
\[X_t \sim Poisson(\alpha t)\]
\[E(X_t)=\alpha t \qquad Var(X_t)=\alpha t\]
\end{exmp}